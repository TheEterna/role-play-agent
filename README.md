# role-play-agent


题目已经list的很详细了
实现点如下：
1. Java + vue 传统前后端架构
2. 利用 AI 做角色扮演，角色扮演的提示词如何来？ 比如 ai 并不了解实时热梗，导致无实时性，so 实现方案最好有且只有唯一，冷缓存角色 + 热搜索 + ai 生成
3. 语音聊天 如何实现，简单实现 浏览器自带转文字传递后端，大模型 输出，tts 文字 -> 语音，这是最快速的方案，效果也最差，微信“按住说话” 比较适合这种实现方式，如果题目中说的语音聊天 是 电话聊天，那就应该直接 调用 云厂商 API， 效果最好，成本最低，最可控，生态最好

以上就是解决方案，据此就能设计出一个基本符合需求的作品

Q&A:
1. actually 我认为这个网页，不具备任何实际生产落地意义，比较适合作为一个AI 工具的附庸，such as 豆包的 “智能体” 广场
2. 基本实现即以上所列三点，据时间长短决定开发功能
3. aliyun，最为普遍，开源较强，模型选择较多，最为熟悉，最为中庸，模型能力比较了解，其dashscope api 的功能也比较契合开发
4. ！！ 应该真正的实现 Agent，这也是我在不断摸索的，简单讲，不只是通过 System Prompt 去简单调控，这并不是真正的智能体，应该深度贯彻角色理念，比如说 一个agent 应该具有自主决策能力，比如 “柯南” 就应该有独有的 api 调用能力，独有的柯南系列知识库，独有的缜密COT Agent 框架，实现真正 role-play 智能体 ！！

### 关键技术： tool/function calling, web, vue/react


语音通话步骤：

1. 前端说话，判断气口（是否结束）技术选型，可以 omni，可以单独api （单独来说，单独效果最可控，但组合费事，组合效果需大量调试），可以浏览器自带（质量最差）
2. 语音传输，根据模型，如果纯 text 模型，需要 ASR 去识别然后表明语气，比如 哼（冷哼一声）
3. 流式回复，如果是纯text模型，就需要 返回text，然后 TTS 去转成 语音

本次选型 Omni ，也是如此实现，使用 websocket 暴露，使用一个缓存区的机制，来保证用户使用不需要进行任何额外存储代码编写，下述 自动模式，手动模式 较为简单，上述流程即可完成
1. 说的话，打的字，录的视频 会直接放到 缓冲区里，有气口时，就直接 sum 在一起，然后发送
2. 全链路封装，相当于 一个系统，多模型整合，当他说话时，user 继续说话，就会继续放在 cache里，然后终止 bot 的数据回复
3. 流式回复，omni 是引入了其他实时的tts

由于 该项目方向与我之前写的一个项目 想要发展的方向接近，所以我将该项目直接接入了我的 一个开源项目，作为一个 独立的 应用广场中的应用存在

[后端](https://github.com/TheEterna/real-agent)

[前端](https://github.com/TheEterna/real-agent-font)
